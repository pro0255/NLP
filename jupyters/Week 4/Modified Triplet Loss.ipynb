{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "casual-federal",
   "metadata": {},
   "source": [
    "# Modified Triplet Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "placed-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "extraordinary-region",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Inputs --\n",
      "v1 : [1. 2. 3.]\n",
      "v2 : [1.  2.  3.5] \n",
      "\n",
      "-- Outputs --\n",
      "cosine similarity : 0.9974086507360697\n"
     ]
    }
   ],
   "source": [
    "# Two vector example\n",
    "# Input data\n",
    "print(\"-- Inputs --\")\n",
    "v1 = np.array([1, 2, 3], dtype=float)\n",
    "v2 = np.array([1, 2, 3.5])  # notice the 3rd element is offset by 0.5\n",
    "### START CODE HERE ###\n",
    "# Try modifying the vector v2 to see how it impacts the cosine similarity\n",
    "# v2 = v1                   # identical vector\n",
    "# v2 = v1 * -1              # opposite vector\n",
    "# v2 = np.array([0,-42,1])  # random example\n",
    "### END CODE HERE ###\n",
    "print(\"v1 :\", v1)\n",
    "print(\"v2 :\", v2, \"\\n\")\n",
    "\n",
    "# Similarity score\n",
    "def cosine_similarity(v1, v2):\n",
    "    numerator = np.dot(v1, v2)\n",
    "    denominator = np.sqrt(np.dot(v1, v1)) * np.sqrt(np.dot(v2, v2))\n",
    "    return numerator / denominator\n",
    "\n",
    "print(\"-- Outputs --\")\n",
    "print(\"cosine similarity :\", cosine_similarity(v1, v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-committee",
   "metadata": {},
   "source": [
    "Now i'll show you how to calculate the similarity scores, using cosine similarity, for 2 batches of vectors. These are rows of individual vectors, just like in the example above, but stacked vertically into a matrix. They would look like the image below for a batch size (row count) of 4 and embedding size (column count) of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "amazing-european",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Inputs --\n",
      "v1 :\n",
      "[[ 1  2  3]\n",
      " [ 9  8  7]\n",
      " [-1 -4 -2]\n",
      " [ 1 -7  2]] \n",
      "\n",
      "v2 :\n",
      "[[ 3.12073214e+00  5.94635976e-01  4.31934989e+00]\n",
      " [ 7.48665925e+00  9.07540069e+00  5.51885926e+00]\n",
      " [-1.88785015e+00 -5.76323054e+00  8.36062683e-03]\n",
      " [ 2.15526770e+00 -7.41437997e+00  5.60537312e-01]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Two batches of vectors example\n",
    "# Input data\n",
    "print(\"-- Inputs --\")\n",
    "v1_1 = np.array([1, 2, 3])\n",
    "v1_2 = np.array([9, 8, 7])\n",
    "v1_3 = np.array([-1, -4, -2])\n",
    "v1_4 = np.array([1, -7, 2])\n",
    "v1 = np.vstack([v1_1, v1_2, v1_3, v1_4])\n",
    "print(\"v1 :\")\n",
    "print(v1, \"\\n\")\n",
    "v2_1 = v1_1 + np.random.normal(0, 2, 3)  # add some noise to create approximate duplicate\n",
    "v2_2 = v1_2 + np.random.normal(0, 2, 3)\n",
    "v2_3 = v1_3 + np.random.normal(0, 2, 3)\n",
    "v2_4 = v1_4 + np.random.normal(0, 2, 3)\n",
    "v2 = np.vstack([v2_1, v2_2, v2_3, v2_4])\n",
    "print(\"v2 :\")\n",
    "print(v2, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "vocal-house",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch sizes match : True \n",
      "\n",
      "-- Outputs --\n",
      "option 1 : loop\n",
      "[[ 0.86072695  0.86778039 -0.5900548  -0.37946969]\n",
      " [ 0.84463842  0.98682511 -0.74628059 -0.33380804]\n",
      " [-0.57539173 -0.92066051  0.89682926  0.74362444]\n",
      " [ 0.19281026 -0.47127181  0.86326245  0.96990756]] \n",
      "\n",
      "option 2 : vec norm & dot product\n",
      "[[ 0.86072695  0.86778039 -0.5900548  -0.37946969]\n",
      " [ 0.84463842  0.98682511 -0.74628059 -0.33380804]\n",
      " [-0.57539173 -0.92066051  0.89682926  0.74362444]\n",
      " [ 0.19281026 -0.47127181  0.86326245  0.96990756]] \n",
      "\n",
      "outputs are the same : True\n"
     ]
    }
   ],
   "source": [
    "# Batch sizes must match\n",
    "b = len(v1)\n",
    "print(\"batch sizes match :\", b == len(v2), \"\\n\")\n",
    "\n",
    "# Similarity scores\n",
    "print(\"-- Outputs --\")\n",
    "# Option 1 : nested loops and the cosine similarity function\n",
    "sim_1 = np.zeros([b, b])  # empty array to take similarity scores\n",
    "# Loop\n",
    "for row in range(0, sim_1.shape[0]):\n",
    "    for col in range(0, sim_1.shape[1]):\n",
    "        sim_1[row, col] = cosine_similarity(v1[row], v2[col])\n",
    "\n",
    "print(\"option 1 : loop\")\n",
    "print(sim_1, \"\\n\")\n",
    "\n",
    "# Option 2 : vector normalization and dot product\n",
    "def norm(x):\n",
    "    return x / np.sqrt(np.sum(x * x, axis=1, keepdims=True))\n",
    "\n",
    "sim_2 = np.dot(norm(v1), norm(v2).T)\n",
    "\n",
    "print(\"option 2 : vec norm & dot product\")\n",
    "print(sim_2, \"\\n\")\n",
    "\n",
    "# Check\n",
    "print(\"outputs are the same :\", np.allclose(sim_1, sim_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "saving-worst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Inputs --\n",
      "sim :\n",
      "[[ 0.9 -0.8  0.3 -0.5]\n",
      " [-0.4  0.5  0.1 -0.1]\n",
      " [ 0.3  0.1 -0.4 -0.8]\n",
      " [-0.5 -0.2 -0.7  0.5]]\n",
      "shape : (4, 4) \n",
      "\n",
      "sim_ap :\n",
      "[[ 0.9  0.   0.   0. ]\n",
      " [ 0.   0.5  0.   0. ]\n",
      " [ 0.   0.  -0.4  0. ]\n",
      " [ 0.   0.   0.   0.5]] \n",
      "\n",
      "sim_an :\n",
      "[[ 0.  -0.8  0.3 -0.5]\n",
      " [-0.4  0.   0.1 -0.1]\n",
      " [ 0.3  0.1  0.  -0.8]\n",
      " [-0.5 -0.2 -0.7  0. ]] \n",
      "\n",
      "-- Outputs --\n",
      "mean_neg :\n",
      "[[-0.33333333]\n",
      " [-0.13333333]\n",
      " [-0.13333333]\n",
      " [-0.46666667]] \n",
      "\n",
      "closest_neg :\n",
      "[[ 0.3]\n",
      " [ 0.1]\n",
      " [-0.8]\n",
      " [-0.2]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hardcoded matrix of similarity scores\n",
    "sim_hardcoded = np.array(\n",
    "    [\n",
    "        [0.9, -0.8, 0.3, -0.5],\n",
    "        [-0.4, 0.5, 0.1, -0.1],\n",
    "        [0.3, 0.1, -0.4, -0.8],\n",
    "        [-0.5, -0.2, -0.7, 0.5],\n",
    "    ]\n",
    ")\n",
    "\n",
    "sim = sim_hardcoded\n",
    "### START CODE HERE ###\n",
    "# Try using different values for the matrix of similarity scores\n",
    "# sim = 2 * np.random.random_sample((b,b)) -1   # random similarity scores between -1 and 1\n",
    "# sim = sim_2                                   # the matrix calculated previously\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Batch size\n",
    "b = sim.shape[0]\n",
    "\n",
    "print(\"-- Inputs --\")\n",
    "print(\"sim :\")\n",
    "print(sim)\n",
    "print(\"shape :\", sim.shape, \"\\n\")\n",
    "\n",
    "# Positives\n",
    "# All the s(A,P) values : similarities from duplicate question pairs (aka Positives)\n",
    "# These are along the diagonal\n",
    "sim_ap = np.diag(sim)\n",
    "print(\"sim_ap :\")\n",
    "print(np.diag(sim_ap), \"\\n\")\n",
    "\n",
    "# Negatives\n",
    "# all the s(A,N) values : similarities the non duplicate question pairs (aka Negatives)\n",
    "# These are in the off diagonals\n",
    "sim_an = sim - np.diag(sim_ap)\n",
    "print(\"sim_an :\")\n",
    "print(sim_an, \"\\n\")\n",
    "\n",
    "print(\"-- Outputs --\")\n",
    "# Mean negative\n",
    "# Average of the s(A,N) values for each row\n",
    "mean_neg = np.sum(sim_an, axis=1, keepdims=True) / (b - 1)\n",
    "print(\"mean_neg :\")\n",
    "print(mean_neg, \"\\n\")\n",
    "\n",
    "# Closest negative\n",
    "# Max s(A,N) that is <= s(A,P) for each row\n",
    "mask_1 = np.identity(b) == 1            # mask to exclude the diagonal\n",
    "mask_2 = sim_an > sim_ap.reshape(b, 1)  # mask to exclude sim_an > sim_ap\n",
    "mask = mask_1 | mask_2\n",
    "sim_an_masked = np.copy(sim_an)         # create a copy to preserve sim_an\n",
    "sim_an_masked[mask] = -2\n",
    "\n",
    "closest_neg = np.max(sim_an_masked, axis=1, keepdims=True)\n",
    "print(\"closest_neg :\")\n",
    "print(closest_neg, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "indian-georgia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Outputs --\n",
      "loss full :\n",
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.51666667]\n",
      " [0.        ]] \n",
      "\n",
      "cost : 0.517\n"
     ]
    }
   ],
   "source": [
    "# Alpha margin\n",
    "alpha = 0.25\n",
    "\n",
    "# Modified triplet loss\n",
    "# Loss 1\n",
    "l_1 = np.maximum(mean_neg - sim_ap.reshape(b, 1) + alpha, 0)\n",
    "# Loss 2\n",
    "l_2 = np.maximum(closest_neg - sim_ap.reshape(b, 1) + alpha, 0)\n",
    "# Loss full\n",
    "l_full = l_1 + l_2\n",
    "# Cost\n",
    "cost = np.sum(l_full)\n",
    "\n",
    "print(\"-- Outputs --\")\n",
    "print(\"loss full :\")\n",
    "print(l_full, \"\\n\")\n",
    "print(\"cost :\", \"{:.3f}\".format(cost))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
